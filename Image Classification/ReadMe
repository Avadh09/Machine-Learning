#Introduction 
The aim of the assignment is to develop a predictive model to classify traffic images for resolving traffic problems. The dataset consists of features extracted from images. There are 887 features which are composed by concatenating various features like histogram oriented features, normalized color histogram, local binary patterns, RGB features, and depth features. The images can be classified as one of the 11 classes: car, SUV, small truck, medium truck, large truck, pedestrian, bus, van, people, bicycle, and motorcycle.

#Approach
The features in the dataset were not consistent. The values were continuous. So, it was important to normalize the values of these features. For normalizing these features, I created a function called normalize in which I dividend each value by the maximum value of the entire dataset. This process converted all the values in the dataset in between 0 and 1. The dataset consists of 887 features. all the features are not required to predict final class. To find necessary features from the dataset, we need to apply some dimensionality reduction algorithm. I used principle component analysis(PCA) for reducing dimensions. For deciding number of components in PCA, I created a variance vs number of components graph. The graph showed that after 40 components, there is no change is variance. So, I decide to use 40 components in PCA algorithm and created a 40-dimension data from 887 features. Although, these 40-dimension dataset is normalized and contains all the useful information, it will not give good prediction as the dataset is highly imbalanced. I tried using the dataset with few algorithms like SVC, naïve Bayes and logistic regression but it was giving very poor result as there were not enough samples for all the classes. This issue can be resolved by sampling the dataset. There are two options for sampling: oversampling and udersampling. I used over sampling for the dataset. I used Synthetic Minority Over Sampling Technique(SMOTE). This method creates synthetic samples using nearest neighbor’s information. It creates samples in such a way that all the classes have same number of samples. We can also choose to create samples for either minority or majority classes.

#Methodology  
First, I used k nearest neighbor algorithm with only normalized dataset which was giving around 0.81 f1 score. I tried changing neighbor count from 3 to 5 and 7 but there was not much difference in f1 score. I tried various other algorithms like Random forest, Support vector machines and multilayer perceptron’s. random forest algorithm was giving me 0.78 f1 score which is also good. Support vector machines were very slow and I was getting very less f1 score. For the Multilayer perceptron’s I tried different combinations of neurons and hidden layer with different activation functions but nothing worked. I got 0.55 f1 score with MLPs. I tried other algorithms like Linear SVM, ExtraTreeClassifiers, decision tress and ridge classifier. All these algorithms were giving me less than 0.5 f1 score. So, I decided that I will train K nearest neighbor and random forest classifier and try to improve f1 score. For overcoming data imbalanced, I used SMOTE before training my model. I tried various combinations of neighbors in K nearest neighbor algorithms with different algorithm to find nearest neighbors like ball tree and kd tree. I was able to get highest 0.8194 f1 score with KNN. Next, I tried random forest and tried changing number of trees in random forest with different methods of splitting tries like sqrt and log. I was able to get more f1 score than KNN. So, I decided that I will work on random forest for final submission. I was able to get 0.8205 f1 score with random forest. I still tried looking for improving my model. After researching, I decided to use Ensemble methods bagging and boosting with random forest. Boosting with random forest was giving me same result as previous approach so I tried bagging with random forest. I was able to get 0.8294 f1 score by using bagging with random forest classifier.
